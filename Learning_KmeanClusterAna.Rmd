---
title: "K-means Cluster Analysis"
output: html_notebook
---

### Replication Requirements
To replicate this tutorial's analysis we need to load the following packages:
```{r message = F}
library(tidyverse) # data manipulation
library(cluster) # clustering algorithms
library(factoextra) # clustering algorithms & visualization
```


### Data preparation
The build-in R dataset *USArrest* is used as demo data.
  - Remove missing data
  - Scale variables to make them comparable
```{r}
# load data
data('USArrests')
my_data <- USArrests

# Remove any missing value (i.e NA values for not available)
my_data <- na.omit(my_data)

# Scale variables
# my_data <- scale(my_data)

# View the first 3 rows
head(my_data)
```

### Clarifying distance measures
The classification of obseravation into group, requires some methods measuring the distance or the (dis)similiarity between the observations. The result of this computation is known as a dissilimarity of distance matrix. There are many methods to calculate this distance information; the choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x,y) is calcuated and it will influence the shape of the clusters.

The choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x,y) is calculated it will influence the shape of the clusters. The classical methods for distance measures are *Euclidean* and *Manhattan distances*, which are defined as follow:
**Euclidean distance**:
      $$d_{euc}(x,y) = \sqrt{\sum_{i=1}^{n}{(x_i-y_i)^2}}$$
      
**Manhattan distance**:
      $$d_{man}(x,y) = \sum_{i=1}^{n}{|x_i-y_i|}$$
      
  Where, x and y are two vectors of length n.

Other dissimilarity measures exist such as correlation-based distances, which is widely used for gene expression data analyses. Correlation-based distance is defined by substracting the correlation coefficient from 1. Different types of correlation methods can be used such as:

**Perason correlation distance**:
      $$d_{cor}(x,y) = 1 - \frac{\sum_{i=1}^n{(x_i-\bar{x})(y_i-\bar{y})}}{\sqrt{\sum_{i=1}^n{(x_i-\bar{x})^2\sum_{i=1}^n(y_i-\bar{y})^2}}}$$
      
**Spearman correlation distance**:
      $$d_{spear}(x,y) = 1 - \frac{\sum_{i=1}^n{(x'_i - \bar{x'})(y'_i-\bar{y'})}}{\sum_{i=1}^n{(x'_i - \bar{x'})^2\sum_{i=1}^n{(y'_i-\bar{y'})^2}}}$$
  Where $x'_i = rank(x_i)$ and $y'_i = rank(y_i)$.
  
**Kendall correlation distance**:
  Kendal correlation method mesuares the correspondence between the ranking of x and y variable. The total number of possible pairings of x with y observations is $n(n-1)/2$, where n is the size of x and y. Begin by ording the pairs by the x values. If x and y are correlated, then they would have the same relative rank orders. Now, for each $y_i$, count the number of $y_j < y_i$ (discordant pairs (d)).
  
  Kendall correlation distance is defined as follow:
    $d_{kend} (x,y) = 1 - \frac{n_c-n_d}{1/2n(n-1)}$
  
  The choice of distance measures is very important, as it has a strong influence on the clstering results. For most common clustering software, the dafault distance measure is the Euclidean distance. However, depending on the type of the data and the research questions, other dissimilarity measures might be preferred and you should be aware of the options.

Within R it is simple to compute and visualize the distance matrix using the function $\color{red}{\text{get_dist}}$ and $\color{red}{\text{fviz_dist}}$ from the $\color{red}{\text{factoextra}}$ R package. This starts to illustrate which states have large dissimilarities(red) versus those that appear to be fairly fimilar(red).
  - $\color{red}{\text{get_dist}}$ : for computing a distance matrix between the rows of a data matrix. The default distance computed is the Euclidean; however,
  - $\color{red}{\text{get_dist}}$ also supports distanced described in equations 2-5 above plus others.
  - $\color{red}{\text{fviz_dist}}$ : for visualizing a distance matrix

```{r}
distance <- get_dist(my_data)
fviz_dist(distance,gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

  
### K-Means Clustering

Commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of _k_ groups (i.e. k clusters), where _k_ represents the number of groups pre-specified by the analyst. It classifies objects in multiple groups (i.e., clusters), such that objects within the same cluster are as similar as possible (i.e., high intra-class similarity), whereas objects from different clusters are as dissimilar as possible (i.e. low inter-class similarity). In K-means clustering, each cluster is represented by its center (i.e, centroid) which corresponds to the mean of points assigned to the cluster.


#### The Basic Idea

The basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized. There are several k-means algorithms available. The standard algorithm is the Hartigan-Wong algorithm (1979), which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid:

$$W(C_k) = \sum_{x_i\in C_k}{(x_i-\mu_k)^2} \ \ \ \ (6)$$
Where:

  - $x_i$ is a data point belonging to the cluster $C_k$
  - $\mu_k$ is the mean value of the points assigned to the cluster $C_k$

Each observation ($x_i$) is assigned to a given cluster such that the sum of squares (SS) distance of the observation to their assigned cluster centers ($\mu_k$) is minimized.

We define the total within-cluster variation as follows:
$$tot.withiness = \sum_{k=1}^k{W(C_k)}=\sum_{k=1}^k{\sum_{x_i\in C_k}{(x_i-\mu_k)^2}} \ \ \ \ \ \ \ (7)$$
The total within-cluter sum of square measure the compachness (i.e goodness) of the clustering and we want it to be as small as possible.


### K-means Algorithm

The first step when using k-means clustering is to indicate the number of clusters(k) that will be generated in the final solution. The algorithm starts by randomly selecting k objects from the data set to serve as the initial centers for the clusters. The selected objects are also knownas cluster means or centroids. Next, each of the remaining objects is assigned to it's closeset centroid, where closest is defined using the Euclidean distance (Eq.1) between the object and the cluster mean. This step is called "cluster assignment step". After the assignment step, the algorithm computes the new mean value of each cluster. The term cluster "centroid update" is used to design this step. Now that the centers have been recalculated, every observation is checked agagin to see if it might be closer to a different cluster. All the objects are reassigned again using the updated cluster means. The cluster assignments stop changeing (i.e. unit convergence is achieved). That is, the clusters formed in the current iteration are the same as those obtained in the previous iteration.

K-means algorithm can be summarized as follows:

  1. Specify the number of clusters(K) to be created (by the analyst)
  2. Select randomly k objects from the data set as the initial cluster centers or means
  3. Assigns each observation to their closest centroid, based on the Euclidean distance between the object and the centroid
  4. For each of the k clusters update the cluster centroid by calculating the new mean values of all the data _p_ containing the means of all variables for the observations in the kth cluster; _p_ is the number of varaibles.
  5. Iteratively minimize the total within sum of square (Eq. 7). That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iteration is reached. By default, the R software use 10 as the default value for the maximum number of iterations.
  

### Computing k-means clustering in R

We can compute k-means in R with the $\color{red}{kmeans}$ function. Here will group the data into two clusters ($\color{red}{centers =2}$). The $\color{red}{kmeans}$ function also has an $\color{red}{nstart}$ option that attempts multiple initial configurations and reports on the best one. For example, adding $\color{red}{nstart = 25}$ will generate 25 initial configurations. This approach is often recommended.

```{r}
k2 <- kmeans(my_data,centers = 2 , nstart = 25)
str(k2)
```

The output of $\color{red}{kmeans}$ is list with serveral bit of information. The most important being:

  - $\color{red}{cluster}$ : A vector of integers (from 1:k) indicating the cluster to which each point is allocated.
  - $\color{red}{centers}$ : A matrix of cluster centers.
  - $\color{red}{totss}$ : The total sum of squares.
  - $\color{red}{withinss}$ : Vector of within-cluster sum of squares, one component per cluster.
  - $\color{red}{tot.withinss}$ : Total within-cluster sum of squares, i.e. sum(withinss).
  - $\color{red}{betweenss}$ : The between-cluster sum of squares, i.e. \$totss-tot.withinss$\.
  - $\color{red}{size}$ : The number of points in each cluster.
  
If we print the result we'll see that our grouping resulted in 2 cluster sizes of 30 and 20. We see the cluster centers (means) for the two groups across the four variables ( _Murder_, _Assault_, _UrbanPop_, _Rape_). We also the cluster assignment for each observation (i.e. Alabama was assigned to cluster 2, Arkansas was assigned to cluster 1, etc.).
```{r}
k2
```

We can also view our results by using $\color{red}{fviz\_cluster}$. This provides a nice illustration of the clusters. If there are more than two dimensions(variables) $\color{red}{fviz\_cluster}$ will perform principal component analysis (PCA) and plot the data points according to the first tow principal components that explain the majority of the variance.
```{r}
fviz_cluster(k2,data = my_data)
```

Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables.

```{r}
my_data %>% as_tibble() %>% mutate(cluster = k2$cluster ,
                                   state = row.names(USArrests)) %>%
  ggplot(aes(UrbanPop,Murder, color = factor(cluster),label = state))+
  geom_text()
```

Because the number of cluster(k) must be set before we start the algorithm, it is often advantageous to use serval different values of k and examine the differences in the results. We can execute the same process for 3,4,and 5 cluster, and the results are shown in the figure:
```{r}
k3 <- kmeans(my_data, centers = 3 ,nstart = 25)
k4 <- kmeans(my_data, centers = 4 , nstart = 25)
k5 <- kmeans(my_data , centers = 5 , nstart =  25)

## plot to compare
p1 <- fviz_cluster(k2,geom = "point",data = my_data) + ggtitle("k =2 ")
p2 <- fviz_cluster(k3, geom = "point" , data = my_data) + ggtitle("k = 3")
p3 <- fviz_cluster(k4,geom = "point",data = my_data) + ggtitle("k =4 ")
p4 <- fviz_cluster(k5, geom = "point" , data = my_data) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1,p2,p3,p4 , nrow = 2)
```

Although this visual assessment tells us where true dilineations occurs (or do not occur such as cluster 2 & 4 in the k = 5 graph) between clusters, it does not tell us what the optimal number of clusters is.


### Determining Optimal Clusters

As you may recall the analyst specifies the number of clusters to use; preferably the analyst would liek to use the optimal number of clusters. To aid the analyst, the following explains the three most popular methods for determining the optimal clusters, which includes:

  1. Elbow method
  2. Silhouette method
  3. Gap statistic
  
#### Elbow Method

Recall that, the basic idea behind cluster partitioning method, such as k-means clustering, is to define clusters suct that the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square) is minimized:
  $$min (\sum_{k=1}^{k}{W(C_k)}) \  \ \    \ \ \ \ \ \ \ (8)$$
Where $C_k$ is $k^{th}$ cluster and $W(C_k)$ is the within-cluster varation. The total within-cluster sum of square (wss) measures the compactness of the clustering and we want it to be as small as possible. Thus, we can use the following algorithm to define the optimal clusters:

  1. Compute clustering algorithm (e.g., k-mean clustering) for different values of k. For instance, by varying k from 1 to 10 clusters
  2. For each k, calculate the total within-cluster sum of square (wss)
  3. Plot the curve of wss according to the number of clusters k.
  4. The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.
  
We can implement this in R with the following code. The results suggest that 4 is the optimal number of clusters as it appears to be the bend in the knee (or elbow)

```{r}
set.seed(123)

# funtion to compute total within-cluster sum of square
wss <- function(k) {
  kmeans(my_data , k , nstart = 10)$tot.withinss
  }
  

# compute the plot wss for k = 1 to k = 15
k.values  <- 1:15

# extract wss for 2 - 15 clusters
wss_values <- map_dbl(k.values,wss)

plot(k.values,wss_values,
     type = 'b' , pch = 19 , frame = F ,
     xlab = "Number of clusters K",
     ylab = "Total within-clusters sum of squares")
```

Fortunately, this process to compute the "Elbow method" has been wrapped up in a single function ($\color{red}{fviz\_nbclust}$):
```{r}
set.seed(123)
fviz_nbclust(my_data,kmeans , method = "wss")
```


#### Average Silhouette Method

In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values of k.

We can use the $\color{red}{silhouette}$ function in the cluster package to compute the average silhouette width. The following code computes this approach for 1-15 clusters. The results how that 2 clusters maximize the average silhouette values with 4 clusters coming in as second optimal number of clusters.

```{r}
avg_sil <- function (k){
  km.res <- kmeans(my_data , centers  = k ,nstart = 25)
  ss <- silhouette(km.res$cluster,dist(my_data))
  mean(ss[,3])
}

# compute and plot was for k =2 to k =15
k.values <- 2:15

# extract avg silhouette for 2 - 15
avg_sil_values <- map_dbl(k.values,avg_sil)

plot(k.values , avg_sil_values, 
     type = 'b', pch = 19 , frame = F,
     xlab = "Number of clusters K",
     ylab = "Average Silhouettes")
```

Similar to the elbow method, this process to compute the "Average silhoutte method" has been wrapped up in the single function ($\color{red}{fviz\_nbclust}$):

```{r}
fviz_nbclust(my_data , kmeans , method = 'silhouette')
```


#### Gap Statistic Method

The gap statistic has been published by [R. Tibshirani, G. Walther, and T. Hastie (Standford University, 2001)](http://web.stanford.edu/~hastie/Papers/gap.pdf). The approach can be applied to any clustering method (i.e. K-means clustering, hierarchical clustering). The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering). The reference dataset is generated using Monte Carlo simulations of the sampling process. That is, for each variable ($x_i$) in the data set we compute its range [$min(x_i),max(x_j)$] and generate values for the n points uniformly from the interval min to max.

For the observed data and the reference data, the total intracluster variation is computed using different values of k. The gap statistic for a given k is defined as follow:
    $$Gap_n(k) = E^*_n log(W_k) - log(W_k) \ \ \ (9)$$
Where $E^*_n$ denotes the expectation under a sample size n from the reference distribution. $E^*_n$ is defined via bootstrapping(B) by generating B copies of the reference datasets and, by computing the average $log(W_k^*)$. The gap statistic measures the deviation of the observed $W_k$ value from its expected value under the null hypothesis. The sestimate of the optimal clusters ($\hat{k}$) will be the value that maximized $Gap_n(k)$. This means that the clustering structure is far away from the uniform distribution of points.

In short, the algorithm involves the following steps:

  1. Cluster the observed data, varying the number of clusters form k = 1, ..., $k_{max}$, and compute the corresponding $W_k$.
  2. Generate B reference data sets and cluster each of them with varying number of clusters k = 1,... , $k_{max}$. Compute the estimated gap statistic presented in eq. 9.
  3. Let $\bar{w} = \frac{1}{B} \sum_b{log(W_{kb}^*)}$, compute the standard deviation $sd(k) = \sqrt{(1/b)\sum_b{(log(W_{kb}^*)-\bar{w})^2}}$ and define $s_k = sd_k \times \sqrt{1+1/B}$.
  4. Choose the number of clusters as the smallest k such that $Gap(k) >= Gap(k+1) - s_{k+1}$
  
To compute the gap statistic method we can use the $\color{red}{clusGap}$ funciton which provides the gap statistic and standard error for an output.

```{r}
set.seed(123)
gap_stat <- clusGap(my_data , FUN = kmeans, nstart = 25 , K.max = 10 , B = 50)
print(gap_stat ,method = "firstmax")
```

We can visualize the results with $\color{red}{fviz\_gap\_stat}$ which suggests four clusters as the optimal number of clusters.

```{r}
fviz_gap_stat(gap_stat)
```

In addition to these commonly used approaches, the $\color{red}{NbClust}$ package, publised by [Charrad et al., 2014](http://www.jstatsoft.org/v61/i06/paper), provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, difference measures, and clustering methods.


### Extracting Results

With most of these approachees suggesting 4 as the number of optimal clusters, we can perform the final analysis and extract the results using 4 clusters.

```{r}
set.seed(123)
final <- kmeans(my_data , 4, nstart = 25)
print(final)
```

We can visualize the results using $\color{red}{fviz\_cluster}$:

```{r}
fviz_cluster(final, data = my_data)
```

And we can extract the clusters and add to our initial data to do some descriptive statistics at the cluster level:

```{r}
USArrests %>%
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")
```

### Additional Comments

K-means clustering is a very simple and fast algorithm.Furthermore, it can effciently deal with very large data sets. However, there are some weaknesses of the k-means approach.

One potential disadvantage of K-means clustering is that is requires us to prespecify the number the number of clusers. Hierarchical clustering is an alternative approach which does not require that we commit to a partcular choice of clusters. Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram. A future tutorial will illustrate the hierachical clustering approach.

An additional disadvantage of K-means is that it's sensitive to outliers and different results can occur if you change the ordering of your data. The Partitioning Around Methoid (PAM) clustering approach is less sensitive to outliers and provides a robust alternative to k-means to deal with these situations. A future tutorial will illustrate the PAM clustering approach.

For now, you can learn more about clustering methods with:

  -[An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
  -[Applied Predictive Modeling](http://appliedpredictivemodeling.com/)
  -[ELements of Statistical Learning](https://statweb.stanford.edu/~tibs/ElemStatLearn/)
  -[A Practical Guide to Cluster Analysis in R](https://www.amazon.com/Practical-Guide-Cluster-Analysis-Unsupervised/dp/1542462703/ref=sr_1_1?ie=UTF8&qid=1493169647&sr=8-1&keywords=practical+guide+to+cluster+analysis)

